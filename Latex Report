\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.75in,vmargin=1.25in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\usepackage{graphicx}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
\usepackage{textcomp}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
% columns=flexible,
upquote=true,
breaklines=true,
showstringspaces=false
}
%  -------------------------------------------- 

%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}

\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (FILL IN THE TABLE AS INSTRUCTED IN THE ASSIGNMENT) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Project \#2}} % <-- replace with correct assignment #

Due: April 28th, 2023 11:59pm EST % <-- replace with correct due date and time
\end{center}
\begin{center}
Group GitHub Link: \href{https://github.com/belldm/IE-332-Project-2}{github.com/belldm/IE-332-Project-2}   
\end{center}
\vspace{0.3in}

\noindent We have {\bf read and understood the assignment instructions}. We certify that the submitted work does not violate any academic misconduct rules, and that it is solely our own work. By listing our names below we acknowledge that any misconduct will result in appropriate consequences. 

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together -- we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Algorithm Dev & Analysis & Implementation & Testing & Report & Overall & DIFF\\
      \hline
      David Bell & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Maude Frappart & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Justin Ha & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Gianna Marzen & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      Kyle Wilson & 20 & 20 & 20 & 20 & 20 & 100 & 0\\
      \hline
      St Dev & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}
%  -----------------------------------------

%  TODO LIST (COMPLETE THE FULL CHECKLIST - USE AS EXAMPLE THE FIRST CHECKED BOXES!) ----------------------

%% LaTeX
% Für alle, die die Schönheit von Wissenschaft anderen zeigen wollen
% For anyone who wants to show the beauty of science to others

%  -----------------------------------------


\begin{document}


\addcoversheet


% BEGIN YOUR ASSIGNMENT HERE:
\newpage

\tableofcontents


\newpage
\section{Design and Planning}
Our objective for this project was to train a voting-based optimization algorithm that performs adversarial attacks on and image classification model. The first step was to develop an understanding of the requirements and expectations, and subsequently develop a plan to accomplish our objective. Using class resources and online text, we began developing ideas and strategies for the five algorithms we would implement in the majority classifier. The original trained model we were given provided a 100\verb|%| accuracy rate for the training data, so our adversarial attacks needed to be robust enough to fool the model. After the testing and verification processes, we were able to identify the five most effective sub-algorithms to implement into the majority classifier. In order to do this we tested several sub-algorithms, ran into dozens of errors, and had to make difficult decisions as to which classifier's would be best implemented. It was essential to develop and select diverse sub-algorithms to best accomplish our goal.


\section{Algorithms}
\subsection{Patterned Pixel Change}
As discussed in class by Carlos, changing pixels to fool the algorithm seemed an obvious choice, even with the limiting budget of .01 total pixels changed in an image. We decided that changing 1 of every 100th pixel to the color white would be the best way to fool the classifier, simply using sequencing and rgb color valued to edit individual pixels.
\begin{lstlisting}[language = R, frame=single]
library(jpeg)

pixel10th <- function (files_to_process) {
  # Process each file
  for (file in files_to_process) {
    # Read in the file
    img <- readJPEG(file)
    
    # dimensions of png
    width <- dim(img)[1]
    height <- dim(img)[2]
    
    # for loop to change the color of every 100th pixel
    for (i in seq(1, width, by=100)) {
      for (j in seq(1, height, by=100)) {
        #change color to white in rgb
        img[i, j, ] <- c(1, 1, 1)
      }
    }
    
    modified_file <- paste0("modified_", file)
    writeJPEG(img, modified_file)
  }
}

# Get a list of all jpgs in dandelion folder
all_files_d <- list.files(path = "dandelions", pattern = "\\.jpg$")
files_dandelions <- all_files_d[!grepl("modified_", all_files_d)]
pixel10th(files_dandelions)

# Get a list of all jpgs in grass folder
all_files_g <- list.files(path = "grass", pattern = "\\.jpg$")
files_grass <- all_files_g[!grepl("modified_", all_files_g)]
pixel10th(files_grass)
\end{lstlisting}

\subsection{Noise}
When brainstorming ideas for ways to fool the binary image classifier, adding noise to the images was an idea our team thought would work. Noise in the context of image alteration is when random variations occur among the pixels. These variations can appear as varying color brightness, image distortion, or just any way additional information is added to the original image that was not originally there. Using noise to fool a binary image classifier often is a good choice because the binary image classifier relies on repeating patterns when it comes to shape and color to be able to accurately identify the right images. Noise has the ability to affect that pattern recognition so it ends up being a good algorithm to use. In the algorithm itself we first set the proportion of pixels that would be modified to 1 percent so that we could meet our project requirement goals. After reading in the images the first thing the algorithm would do would be to choose a random assortment of pixels to modify based on our 1 percent parameter. Next it would modify the pixels it had chosen by creating a copy of the image and then adding a normally distributed amount of noise to the copy of the image. Lastly after modifying the image it would save the modified image to a folder with a name post fix of noisy. The specific R packages we used in this algorithm were library(jpeg) and library(magick). The jpeg package was used to read in the images, and the magick package is used to write modified image matrix to the new file. All the other functions used were built into R. 
\begin{lstlisting}[language = R, frame = single]
add_noise_to_images <- function(path) {
library(jpeg)
library(magick)
  
# get the list of all JPEG files in the folder
filelist <- list.files(path, pattern = ".jpg", full.names = TRUE)
  
# set the proportion of pixels to modify
proportion <- 0.01
  
# iterate over each file in the folder
for (file in filelist) {
# read the image
img <- readJPEG(file)
# identify random pixels to modify
num_pixels <- length(img)
num_to_modify <- round(num_pixels * proportion)
idx_to_modify <- sample(num_pixels, num_to_modify)
    
# modify the identified pixels
noisy_matrix <- img
noisy_matrix[idx_to_modify] <- noisy_matrix[idx_to_modify] + rnorm(num_to_modify, mean = 0, sd = 0.1)
# write the filtered image to a new file
  new_file <- sub(".jpg", "_noisy1%.jpg", file)
  writeJPEG(noisy_matrix, quality = 100, new_file)
  }
}

add_noise_to_images("dandelions")
add_noise_to_images("grass")
\end{lstlisting}

\subsection{Spatial Transformation Attack}
Another algorithm our group attempted was a spatial transformation attack by rotating the images vertically in an attempt to fool the binary image classifier. By rotating, we are not changing the pixels, but transforming them to fit the given requirements. Vertically flipping an image was done to fool the algorithm because by changing the orientation of an image, the classifier misinterprets the contents of the image because the items within the image are altered and flipped which allows us to successfully fool it.  We have done this by utilizing the \verb|'magick'| library and the \verb|image_flip()| function. We used this function in two FOR loops for both the dandelion and grass folder. The for loop runs through the pictures in the folder and creates a new copy of the image with the desired vertical rotation.
\begin{lstlisting}[language = R, frame=single]
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
library(magick)
library(jpeg)
install_tensorflow(extra_packages="pillow")
install_keras()

path <- "[corresponding picture folder]"
filelist <- list.files(path, pattern = ".jpg", full.names = TRUE)

for (file in filelist) {
  img <- image_read(file)
  vert_flip <- image_flip(img)
  
  new_file <- sub(".jpg", "_vertflip.jpg", file)
  image_write(vert_flip, new_file)
}
\end{lstlisting}
\subsection{Centered Pixel Box}
Once we had developed our Patterned Pixel color changing algorithm for 1 in every 100 pixels, developing a centered black box of pixels to fool the model was a simpler endeavour. The decision to include this algorithm stemmed from it's effectiveness. Using the \verb|'jpeg'| library and the algorithm uses a for loop to read the images, define the dimensions of the \verb|jpeg|, define the dimensions of the centered box, and finally setting the box to black, while only using 1\verb|%| of the image's pixels. Example code for adding a black box to the images of dandelions: 
\begin{lstlisting}[language = R, frame = single]
library(jpeg)
# Get a list of all JPG files in the "dandelions" folder
file_list <- list.files(path = "dandelions", pattern = "\\.jpg$", full.names = TRUE
# Loop over each file in the list
for (file in file_list) {
  # Read the image
  image <- readJPEG(file)
  # dimensions of jpg
  width <- dim(image)[1]
  height <- dim(image)[2]
  # define the dimensions of the black box
  box_width <- floor(width / 10)
  box_height <- floor(height / 10)
  x_center <- floor(width / 2)
  y_center <- floor(height / 2)
  # set the pixels inside the black box to black
  for (i in seq(x_center - floor(box_width / 2), x_center + floor(box_width / 2))) {
    for (j in seq(y_center - floor(box_height / 2), y_center + floor(box_height / 2))) {
      image[i, j, ] <- c(0, 0, 0) 
    }
  }
  # Write the modified image to a new file
  writeJPEG(image, paste0(file, "_modified.jpg"))
}

\end{lstlisting}
\subsection{Blur}
The blur effect was attempted in several ways, finally resulting in our group using the \verb|'isoblur'| function through the \verb|'imager'| package in R. Our group felt using a blur or smoothing filter would adequately fool the model. To do this, the algorithm looped through all the files, read the images, applied the \verb|'isoblur'| to the images, and saved the new modified image into the folders. We did run into some issues with it's performance (See Appendix III), but found that it was one of the most effective and efficient working sub-algorithms we had developed. 
\begin{lstlisting}[language = R, frame = single]
library(imager)
# Set the path to your image folder
img_folder <- "C:/kylewilson/Desktop/332/dandelions"

# Loop over all files in the folder with the .jpg extension
for (file in list.files(img_folder, pattern = "\\.jpg$")) {
  
  # Read in the image
  img_int <- imager::load.image(file.path(img_folder, file))
  
  # Apply iso blur
  blurred_img_int <- imager::isoblur(img, sigma = .01)
  
  # Create a new filename for the blurred image
  new_file <- gsub("\\.jpg$", "_blurred.jpg", file)
  
  # Write the blurred image to a new file
  imager::save.image(blurred_img, file.path(img_folder, new_file))
}
\end{lstlisting}

\section{Majority Classifier}
\begin{lstlisting}[language = R, frame = single]
# load required libraries for each individual algorithm

# define the image classifier and the image-changing algorithms
classifier <- function(image) {
  # classifier code here (given)
}

# change 100th pixel to white
pixel10th <- function(path) {
  # code from 2.1 here 
}

# vertical flip algorithm
vertical_flip <- function(path) {
  # code from 2.3 here
}

# blur the image
isoblurFunction <- function(path) {
  # code from 2.5 here
}

# place a black box in the center of the image, covering 1% of pixels
centered_black_box <- function(path){
  # code from 2.4 here
}

# add noise to 1% of the image
add_noise_to_images <- function(path) {
  # code from 2.2
}

# load the images from each folder
all_files_d <- list.files(path = "dandelions", pattern = "\\.jpg$", full.names = TRUE)
files_dandelions <- all_files_d[!grepl(ignore_patterns, all_files_d)]
all_files_g <- list.files(path = "dandelions", pattern = "\\.jpg$", full.names = TRUE)
files_grass <- all_files_g[!grepl(ignore_patterns, all_files_g)]

# weights range from .075 to .325 for each algorithm
algorithm_weights <- c(0.075, 0.125, 0.2, 0.275, 0.325)

# apply the predictor on each image
best_func <- function(files1, files2) {
  for (file in files){
      image <- load.image(image_file)
      
      # determine best algorithm for each image
      predictions <- map_dbl(list(pixel10th, vertical_flip, isoblurFunction, centered_black_box, add_noise_to_images), function(algorithm) {
        modified_image <- algorithm(image)
        classifier(modified_image)
      })
      
      # calculate the weighted sum of probabilities for each algorithm
      probs <- c(0,0,0,0,0)
      for (i in seq_along(predictions)) {
        probs[predictions[i]] <- probs[predictions[i]] + algorithm_weights[i]
      }
      
      # return the algorithm with highest weighted sum of probabilities
      return(which.max(probs))
    }
}

best_func <- (files_grass, files_dandelions)
\end{lstlisting}

\noindent While we could not successfully combine the 5 algorithms into a majority voting classifier, this method was our general/desired approach. We converted each of our algorithms into functions that would accept an argument of the folder path. Then they would take the path and make a list of the files, and ignore previous iterations on the images (each change would only modify the original images). The predictor would consider each folder (dandelions and grass) separately, performing the algorithms on all images and determine which algorithm would cause the classifier to predict the least amount of images correctly (as desired when fooling the classifier). It would then assign weights to each algorithm to give the best algorithm for that image the highest weight so that the classifier is more likely to be fooled. It will then calculate the sum of probabilities depending on weights and return the best algorithm. This approach would best perform the tasks expected and allow for the algorithm to be more likely to be fooled than if we simply tried to use a single algorithm or the same algorithm on each.

\newpage
\section{Appendix I: Testing}
\subsection{Correctness and Verification}
For testing our algorithms, we utilized the RStudio to identify if our algorithm was capable of fooling the model. To do this, we needed to set the appropriate directories, install and call appropriate packages containing desired functions, run our fooling algorithms, followed by running the classifier algorithm we were given. Once this happened RStudio produced results detailing which images, when modified, were able to fool the classifier algorithm. This provided us the ability to identify which algorithms were working, and more specifically how effective they were in accomplishing our objective. 
\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - normal.jpg}
    \caption{Dandelion Picture - Original}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - 1pernoise.jpg}
    \caption{Dandelion Picture - 1 Percent Noise}
  \end{minipage}\hfill
  \centering
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - pixel modif.jpg}
    \caption{Dandelion Picture - Modified Pixels}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - vert flip.jpg}
    \caption{Dandelion Picture - Vertical Flip}
  \end{minipage}
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - Box.png}
    \caption{Dandelion Picture - Box}
  \end{minipage}
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{images/Dandelion - Blur.jpg}
    \caption{Dandelion Picture - Blur}
  \end{minipage}
\end{figure}
\newpage


\newpage
\section{Appendix II: Run-time Complexity and WallTime}
\subsection{Technical Challenges}
One major issue in regards to accounting for run-time complexity and WallTime was the fact that we had difficulties in developing working algorithms and also developing the majority classifier. This created challenges because the project was set up for us to be able to easily compare run times for the different algorithms/find an average run time for the total modification of pictures and their classification, but the lack of a working majority voting classifier algorithm prevented the ability of that. We were able to account for the run-time complexities and walltimes of each individual algorithm separately but did not get a combined time for the overall average.
\subsection{Run-time Complexity}
For the vertical flip algorithm, the FOR loop runs a total of n times which is the number of pictures in the selected folder. The code is linear as it looks through each picture in the folder once. Therefore, the time complexity of the vertical flip algorithm is: 
\begin{center}
    = $\Theta$(n) 
\end{center}

\noindent For the Centered Black Box algorithm, the run-time complexity of the algorithm depends on the number and size of the images in the directories. The 'list.files' function has a time complexity of O(n), since it needs to scan the directory to find all the files that match the pattern. The 'readJPEG' function reads in the image and has a time complexity of O(m), since it needs to read in each pixel of the image. The nested loops that modify the pixels have a time complexity of O(m), since each pixel in the black box is being modified. The 'writeJPEG' function writes the modified image to a new file and has a time complexity of O(m), since it needs to write out each pixel of the image. Therefor the overall time complexity of the algorithm is: 
\begin{center}
    = O($n*m^2$)
\end{center}

\noindent For the \verb|'blur'| algorithm, the time complexity depends on the number of images in the dandelion and grass folders. The \verb|'isoblurFunction()'| function loops through each file, so the time complexity of that operation is \verb|O(n)|. Inside the loop, the \verb|'load.image()'| function has a time complexity of O(1) since it simply reads in the image from the file. The \verb|isoblur()| function applies a blur to the image, which has a time complexity of O(w * h * r), where w and h are the width and height of the image, and r is the radius of the blur. Since the radius is fixed at 5 pixels, we it represents a constant factor. Therefore, the time complexity of the \verb|'isoblur()'| function can be simplified to O(w * h). Finally, the \verb|'save.image()'| function has a time complexity of O(w * h), as it needs to write the image data to disk. Therefore, the overall time complexity is: 
\begin{center}
    = O($n * w * h$)
\end{center}


\noindent For the 100th pixel change algorithm, \verb|pixel10th|, the run-time complexity mostly relies on the amount of pixels of each image and the total files that are to be modified in each of the dandelions and grass folders. The function has to loop through the pixels, by 100 pixels per row and column, which is why the complexity accounts for O(m), where m is the total number of pixels per image. Then the function will perform this action on all of the images in the folder, thus O(n) will be the other added complexity with n being the total files in the folder, or combined amount of files in both the grass and dandelion folders. The complete run-time complexity algorithm for this is thus:
\begin{center}
    = O($n*m$)
\end{center}

\noindent The Noise algorithm iterates over each JPEG file in the folder, reads the image, modifies a proportion of randomly selected pixels, and writes the resulting noisy image to a new file. Since the algorithm processes each file in the directory, its overall time complexity is O(n * m), where n is the number of files and m is the size of each file. The complexity of the pixel modification step is dependent on the proportion of pixels to modify, which is set to a constant value of 0.01 in the algorithm. Therefore, the time complexity of the algorithm can be simplified to:
\begin{center}
    = O(n)
\end{center}

\begin{lstlisting}[language = R, frame = single]
#example of modified grass images getting kicked by classifier
1/1 [==============================] - 0s 498ms/step
[1] "(GRASS+2)+turf+lawn+green+ground+field+seamless+texture+2048x2048-2663209674_noisy1%.jpg"
1/1 [==============================] - 0s 56ms/step
[1] "1602408-2700088754_noisy1%.jpg"
1/1 [==============================] - 0s 57ms/step
[1] "18-4196734595_noisy1%.jpg"
1/1 [==============================] - 0s 58ms/step
[1] "20334a31c970fb3607b17613614f9244-1240575823_noisy1%.jpg"
1/1 [==============================] - 0s 57ms/step
[1] "5_noisy1%.jpg"
1/1 [==============================] - 0s 56ms/step

#example of modified dandelion images getting kicked by classifier
1/1 [==============================] - 0s 57ms/step
[1] "636597665741397587-dandelion-1097518082_noisy1%.jpg"
1/1 [==============================] - 0s 55ms/step
[1] "AdobeStock_206345546-426480574_noisy1%.jpg"
1/1 [==============================] - 0s 57ms/step
[1] "common-dandelion-43x_pam3-3575848235_noisy1%.jpg"
1/1 [==============================] - 0s 57ms/step
[1] "dandelion-1307475185dYm-3246239646_noisy1%.jpg"
1/1 [==============================] - 0s 58ms/step
[1] "dandelion-1336930590TbT-2982997207_noisy1%.jpg"
1/1 [==============================] - 0s 56ms/step
\end{lstlisting}
\noindent Above is an example output from running our modified images through the classifier algorithm. More specifically this is the output of the grass and dandelion test of 1 percent noise algorithm. For this algorithm, there was an overall average of 61.5ms taken for each image, including the outlier of the first image. The others are in a similar range, though this is the fastest overall. All of our algorithms produced output from the classifier, but there were too many outputs from all the algorithms to be able to include them in the report here. All of the outputs are in the GitHub. \\


\newpage
\section{Appendix III: Performance}
Performance for this project was crucial, though a challenge to achieve and maintain. We had to make trade-offs through each iteration of the algorithm, with the goal of not only fooling the model through individual sub-algorithms but making each algorithm applicable to a majority classifier. To accomplish this, we wanted to devise algorithms that were diverse, in that they could not only perform but perform differently. We devised several sub-algorithms designed to fool the model, all with different rates of performance and versatility. We felt that if we were able to design algorithms that were more diversified it would also diversify the number of modified images that could fool the model. We also had constraints that needed to be met, like a pixel change budget of 1\verb|%|. We ultimately achieved the budget for 4 of our working algorithms, but were unable to account for the budget when designed our\verb|'blur'| algorithm (mentioned below).

\subsection{Technical Challenges}
\subsubsection{Blur}
An issue that we ran into and were unable to solve, was a pinpointed blur attack on the images. We attempted dozens of renditions of the blur effect using different functions and packages, but ran into cycles of errors each time. we tried functions including \verb|'gaussian_blur'| from the \verb|'imager'| package, \verb|'blur'| function from the \verb|'imager'| and \verb|'EBImage'| package, and the \verb|'blurImage'| function from the \verb|'imager'| package. All of these involved creating a blur at random pinpointed pixels, with a specified blur radius. This was all in an attempt to blur only 1\verb|%| of the pixels rather than the whole image. After countless attempts, we decided to simply use the \verb|'isoblu'| function from the \verb|'imager'| package to create a blur effect for the entire picture (See Section 2.5 for detailed algorithm). 
\subsubsection{Grass Declassification}
Our biggest challenge was our algorithms inconsistent ability to declassify the grass images from the trained model. Through each rendition of our sub-algorithms we found the classifier was rarely fooled in regard to the grass images. Almost every time the model was able to classify the pictures of grass as grass, regardless of the sub-algorithm used. In an attempt to understand why our algorithms weren't able to fool the model consistently, we tried multiple color changing algorithms to attempt to inebriate the model. Even with a full grey-scale effect, full color changes, and multiple orientation transformation algorithms, in addition to our five sub-algorithms, we were unable to fool the model consistently. We deducted that we would be unable to do so, and tried to focus our efforts on the most effective algorithms for fooling the model's dandelion classifications. It's important to note that we were able to fool the grass on multiple occasions, and we were able to fool the grass consistently when multiple algorithms were imposed on the images.
\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - normal.jpg}
    \caption{Grass Picture - Original}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - 1pernoise.jpg}
    \caption{Grass Picture - 1 Percent Noise}
  \end{minipage}\hfill
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - pixel modif.jpg}
    \caption{Grass Picture - Modified Pixels}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - vert flip.jpg}
    \caption{Grass Picture - Vertical Flip}
  \end{minipage}
   \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - Box.jpg}
    \caption{Grass Picture - Box}
  \end{minipage}
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/Grass - Blur.jpg}
    \caption{Grass Picture - Blur}
  \end{minipage}
\end{figure}
\newpage
\section{Appendix IV: Justification}
\subsection{Technical Challenges}
Throughout this project, we tested more than just the 5 algorithms we inevitably chose to work. Ultimately, part of the decision to use said algorithms was influenced by the challenges we ran into while developing and testing other algorithms we had designed. 
\subsubsection{Universal Adversarial Perturbations - (UAP)}
 One of the first machine learning algorithms we tested was the adversarial attack, UAP (Universal Adversarial Perturbation). This attack was meant to add a small perturbation to the large amount of images data, causing the model to declassify them. Through the creation of that algorithm, we ran into multiple issues. First off, this algorithm works best with the “adversarial” function package, which, is not available in R. We tried to use the “reticulate” and “py install” functions to fix that issue, which didn’t work, and even though we ended up using the “cleverhans” package, not being able to use adversarial made the overall process much harder. Therefore, working in R instead of Python was definitely one of our biggest challenges during this project. Another issue we ran into was the "imager" library not working properly on Mac, we downloaded the “XQuartz” software to be able to use the package properly. Finally, the fgsm function included in the UAP code ended up being very complex to work with, as it gave us many errors we weren’t able to get over. Below is the attached code of the UAP algorithm we tried out.

\begin{lstlisting}[language = R, frame=single]
library(reticulate)
reticulate::py_install("cleverhans", force= TRUE)
library(imager)
library(tensorflow)
library(keras)

# Define the paths to your dataset
grass_data_dir <- "/Users/maudefrappart/Library/CloudStorage/OneDrive-purdue.edu/IE332/grass"
dandelions_data_dir <- "/Users/maudefrappart/Library/CloudStorage/OneDrive-purdue.edu/IE332/dandelions"

# Create the data generators for each dataset
img_gen <- image_data_generator()
grass_data <- flow_images_from_directory(
  grass_data_dir,
  target_size = c(224, 224),
  batch_size = 32,
  class_mode = "binary",
  shuffle = TRUE
)
dandelions_data <- flow_images_from_directory(
  dandelions_data_dir,
  target_size = c(224, 224),
  batch_size = 32,
  class_mode = "binary",
  shuffle = TRUE
)

# Define the model you want to attack
model_path <- "/Users/maudefrappart/Library/CloudStorage/OneDrive-purdue.edu/IE332/classifiermodel.R"
model <- tensorflow::tf$keras$models$load_model(model_path)

# Define the parameters for the attack
nb_epochs <- 10
eps <- 0.5
batch_size <- 32
delta <- 0.5

# Create the UAP object
up <- universal_perturbation_fgsm(
  model=model,
  eps=eps,
  delta=delta,
  max_iter=nb_epochs,
  batch_size=batch_size
)

# Generate the UAP
up_fit <- fit_up_generator(grass_data, up)

# Apply the UAP to your datasets
X_grass <- grass_data$X
Y_grass <- grass_data$Y
X_grass_adv <- predict(up_fit, X_grass)

X_dandelions <- dandelions_data$X
Y_dandelions <- dandelions_data$Y
X_dandelions_adv <- predict(up_fit, X_dandelions)

\end{lstlisting}
\subsubsection{Fast Gradient Sign Method - (FGSM)}
The second of the main machine learning algorithms that our team tried was Fast Gradient Sign Method or FGSM. The main goal of FGSM is to use the gradient of the classifying algorithm to create adversarial examples that will fool the classifier. These adversarial examples visually look similar to the original images but fool the classifier not by visually fooling it but by changing the pixel values to fool the algorithm. The FGSM algorithm uses the gradient of the loss function with respect to the input image and then adds small perturbations to the image. This is done by adding a small epsilon to the sign of the gradient. FGSM is an effective adversarial method because it effects the way the classifier classifies images. FGSM works well against binary image classifier like the one in this project because these classifier algorithms are often visual pattern recognition models and the FGSM algorithm can easily modify pixel values in a way that will cause the binary classifier to miss-classify even though the images are visually similar. While FGSM is a relatively easy algorithm to create in Python; in R, our group ran into endless issues when trying to create this FGSM algorithm. Certain packages like "adversarial" "applications" "cleverhans" and even some of the functions in "keras" and "tensorflow" just do not exist in the versions of R we were using and had access to. When it came to loading and pre-processing the images some packages had ways to process and scale the images, but didn't have a way to run other parts and since the were processed using a functions from one package trying to add perturbations with functions from another package did not work. This is because the functions used to process the images were from the keras package in R and the functions used to do the actual FGSM part were based in Python and Rstudio could not handle the overlap. We tried a variety of approaches using different packages and functions that we found in our research, however with each fix a new error popped up. The most common errors were: "error during wrapup:'function we tried' is not an exported object from 'namespace:keras.' This error occured with namespace:tensorflow as well. "Error during wrapup: object 'variable not ran because function didnt exist' not found." This always occurred because the code failed earlier on. Overall the functions in tensorflow and keras that were essential to be able to use to create the FGSM algorithm either did not exist or did not work in Rstudio leading our team to be at an impass for completing that algorithm. Each member of our team tackled this algorithm to try to make it work because we felt we were really close to getting it correc, however none of us were able to solve the errors and we all ended up in an endless error loop. 

\begin{lstlisting}[language = R, frame = single]
install.packages("tensorflow")
install.packages("keras")
install.packages("applications")
library(reticulate)
reticulate:: py_install("pillow", force = TRUE)
reticulate:: py_install("cleverhans", force = TRUE)
library(tidyverse)
library(keras)
library(tensorflow)
use_backend("tensorflow")
library(purrr)

# Load the binary image classifier model
model <- load_model_tf("./dandelion_model")

#load and preprocess a range of sample images from the folder dandelion
img_path <- "./dandelions/"
img <- list.files(img_path, full.names = TRUE)
img <- map(img, ~ keras::image_load(.x, target_size = c(224, 224)))
img <- map(img, keras::image_to_array)
img <- map(img, function(x) x / 255)  # scale pixel values to [0, 1]
img <- map(img, array_reshape, c(1, 224, 224, 3)) # reshape images to expected shape
img <- do.call(rbind, img)

#load and preprocess a range of sample images from the folder grass
img_path2 <- "./grass/"
img2 <- list.files(img_path2, full.names = TRUE)
img2 <- map(img2, ~ keras::image_load(.x, target_size = c(224, 224)))
img2 <- map(img2, keras::image_to_array)
img2 <- map(img2, function(x) x / 255)  # scale pixel values to [0, 1]
img2 <- map(img2, array_reshape, c(1, 224, 224, 3)) # reshape images to expected shape
img2 <- do.call(rbind, img2)

# use fgsm to fool the classifier model
fgsm_attack <- function(model, x, epsilon) {
  # Preprocess the image data
  x <- keras::preprocess_input(x)
  # Compute the gradient of the loss function with respect to the input image
  grad <- k_gradient(model$output, model$input)
  # Compute the sign of the gradient
  sign_grad <- sign(grad)
  # Compute the perturbed image by adding a small epsilon to the sign of the gradient
  x_adv <- x + epsilon * sign_grad
  # Clip the perturbed image to ensure that pixel values remain within [-1, 1]
  x_adv <- clip(x_adv, -1, 1)
  # Reverse the preprocessing step to obtain the perturbed image in its original format
  x_adv <- keras::preprocess_input(x_adv, mode = "tf")
  # Return the perturbed image
  return(x_adv)
}

# generate adversarial examples using FGSM and evaluate the model's accuracy
epsilon <- 0.05 # adjust the value of epsilon to control the strength of the attack
adv_img <- map(img, ~ fgsm_attack(model, .x, epsilon))
adv_img2 <- map(img2, ~ fgsm_attack(model, .x, epsilon))
pred_adv <- predict(model, adv_img)
pred_adv2 <- predict(model, adv_img2)

# calculate the accuracy of the model on original vs. adversarial examples
acc_orig <- mean(as.numeric(pred1 > 0.5))
acc_adv <- mean(as.numeric(pred_adv > 0.5))
cat(paste0("Accuracy on original dandelion images: ", acc_orig, "\n"))
cat(paste0("Accuracy on adversarial dandelion images: ", acc_adv, "\n"))
\end{lstlisting}

\subsubsection{Deepfool}
Deepfool is an algorithm that is similar to FGSM as it adds small perturbations that are not visibly noticeable but causes the binary image classifier to be fooled by the image the Deepfool algorithm makes. As in the name Deepfool algorithms look to fool deep neural networks. As in FGSM, Deepfool computes the gradient with respect to the input image but differs from FGSM in that it iterates the perturbations from a small amount until the minimum amount needed to fool the classifier. As in the FGSM algorithm we ran into issues due to the use of Python functionalities in R. Some of the functions needed were not in the R version of tensorflow and keras and when we tried to update the packages to a new version nothing worked even though the functions were supposed to be there. In the code below we first loaded the model, then created a function to generate the adversarial images, then created a copy of the image, iterated until an adversarial image is found, keep track of the perturbations, apply the perturbations to the image, check to see if the perturbations make the image adversarial, if it is adversarial then stop iterating and return the perturbed image, and then generate the image to view. \\

\begin{lstlisting}[language = R, frame = single]
install.packages("tensorflow", version = "2.3.1")
install.packages("keras", version = "2.4.3")
reticulate:: py_install("pillow", force = TRUE)

library(tidyverse)
library(reticulate)
library(keras)
library(tensorflow)
reticulate:: py_install("cleverhans", force = TRUE)
use_backend("tensorflow")
library(purrr)

# Load and preprocess an image to attack
img_path <- "dandelions/636597665741397587-dandelion-1097518082.jpg"
img <- keras::image_load(img_path, target_size = c(224, 224))
img <- keras::image_to_array(img)
img <- array_reshape(img, c(1, dim(img)))

# Load the binary image classifier model
model <- load_model_tf("./dandelion_model")

# Define a function to generate adversarial images using DeepFool
deepfool_attack <- function(model, image, max_iter = 50, epsilon = 0.02, clip_min = 0, clip_max = 1) {
  # Convert image to a tensor
  image_tensor <- array_reshape(image, dim = c(1, dim(image)))
  image_tensor <- tf$constant(image_tensor)
  
  # Remove the first dimension of the image tensor
  image_tensor <- tf$squeeze(image_tensor, axis = 0)
  
  # Predict the class probabilities for the input image
  preds <- model(image_tensor)
  
  
  # Create a copy of the input image for perturbation
  x_adv <- tf$Variable(image_tensor)
  
  # Get the number of classes in the model
  num_classes <- dim(model$output_shape)[2]
  
  # Iterate until the maximum number of iterations or until an adversarial image is found
  for (iter in 1:max_iter) {
    with(tf$GradientTape() %as% tape, {
      tape$watch(x_adv)
      preds <- model(x_adv)
      pred_class <- tf$argmax(preds, axis = 1)
      y_true <- tf$one_hot(tf$cast(pred_class, dtype=tf$int64), num_classes)
      y_true <- tf$reshape(y_true, shape = c(1, num_classes))
      loss <- keras$backend$categorical_crossentropy(y_true, preds)
      grads <- tape$gradient(loss, x_adv)
    })
    
    # Find the direction of greatest sensitivity to the decision boundary
    w_norm <- tf$linalg$norm(tf$reshape(grads, shape = c(-1, num_classes)), axis = 1)
    f_xw <- model(x_adv)
    f_xw_argmax <- tf$argmax(f_xw, axis = 1)
    
    min_dist <- tf$ones(shape = c(1)) * tf$cast(Inf, dtype = tf$float32)
    min_perturbation <- tf$zeros(shape = dim(image_tensor), dtype = tf$float32)
    
    for (k in 1:(num_classes - 1)) {
      mask <- tf$not_equal(f_xw_argmax, k)
      w_k_norm <- tf$boolean_mask(w_norm, mask)
      f_k_xw <- tf$boolean_mask(f_xw, mask)
      perturbation_k <- (w_k_norm / tf$linalg$norm(w_k_norm)) * (tf$abs(f_k_xw - f_xw[,k]) / w_k_norm)
      perturbation_k <- tf$expand_dims(perturbation_k, axis = 1)
      perturbation_k <- tf$reshape(perturbation_k, shape = dim(image_tensor))
      
      # Keep track of the perturbation with the smallest l2 distance
      dist_k <- tf$linalg$norm(perturbation_k)
      is_smaller_dist <- tf$less(dist_k, min_dist)
      min_dist <- tf$where(is_smaller_dist, dist_k, min_dist)
      min_perturbation <- tf$where(is_smaller_dist, perturbation_k, min_perturbation)
    }
    
    # Apply the perturbation to the image
    x_adv <- tf$clip_by_value(x_adv + min_perturbation, clip_min, clip_max)
    
    # Check if the perturbed image is adversarial
    preds_adv <- model(x_adv)
    pred_class_adv <- tf$cast(tf$argmax(preds_adv, axis = 1), dtype = tf$int32)
    if (tf$equal(pred_class, pred_class_adv)$numpy()) {
      # If the perturbed image is not adversarial, continue to the next iteration
      next
    } else {
      # If the perturbed image is adversarial, stop iterating and return the adversarial image
      adv_image <- keras::array_to_img(x_adv$numpy(), scale = TRUE)
      adv_path <- "dandelions/adv_dandelion.jpg"
      keras::save_img(adv_path, adv_image)
      message("Adversarial image generated and saved to ", adv_path)
      return(adv_image)
    }
  }
  
  # If the maximum number of iterations is reached and no adversarial image is found, return NULL
  message("Maximum number of iterations reached. Failed to generate an adversarial image.")
  return(NULL)
}

# Generate an adversarial image using the DeepFool algorithm
adv_image <- deepfool_attack(model, img)

# Show the original image and the adversarial image side by side
original_image <- keras::array_to_img(img, scale = TRUE)
grid.arrange(original_image, adv_image, ncol = 2, widths = c(1, 1))
\end{lstlisting}


\newpage
\subsection{References}
\begin{enumerate}
     \item  \emph{How to build your own image recognition app with R!} R-Bloggers. (2021, March 16). Retrieved April 27, 2023, from https://www.r-bloggers.com/2021/03/how-to-build-your-own-image-recognition-app-with-r-part-1/ \\
\item  \emph{Image smoothing in R.}Stack Overflow. Retrieved April 27, 2023, from https://stackoverflow.com/questions/\\
17022379/image-smoothing-in-r \\\\
\item \emph{Change image pixel colors in R and save image}. Stack Overflow. Retrieved April 27, 2023, from https://stackoverflow.com/questions/59861122/change-image-pixel-colors-in-r-and-save-image \\\\
\item  \emph{Blur: Apply Gaussian Blur to a pixel image. }RDocumentation. (n.d.). Retrieved April 27, 2023, from https://www.rdocumentation.org/packages/spatstat/versions/1.64-1/topics/blur \\\\
\item \emph{Package Package - Cran.r-Project.org.} Retrieved April 27, 2023, from https://cran.r-project.org/web/packages /magick/magick.pdf. \\\\
\end{enumerate}


\end{document}
